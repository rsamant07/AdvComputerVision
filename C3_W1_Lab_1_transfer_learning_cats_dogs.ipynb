{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C3_W1_Lab_1_transfer_learning_cats_dogs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsamant07/AdvComputerVision/blob/main/C3_W1_Lab_1_transfer_learning_cats_dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1eSA-lkVoBB"
      },
      "source": [
        "# Basic transfer learning with cats and dogs data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcCn1Hr9VoBB"
      },
      "source": [
        "## Run in Google Colab: GPU setup\n",
        "The training portion of this notebook will be faster in google colab using a GPU. \n",
        "- You can click on the \"open in colab\" link above to open the notebook in a colab.\n",
        "- Within the Colab, go to the colab's menu \"Runtime\" -> \"Change runtime type\".  A popup will appear with a menu to select \"None\", \"GPU\" or \"TPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bwkeg5vVoBC"
      },
      "source": [
        "### Import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLbtB3uGKPX"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZks8Gh9VoBG"
      },
      "source": [
        "### Import modules and download the cats and dogs dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y23ucAFLoHop"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "data_url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\"\n",
        "data_file_name = \"catsdogs.zip\"\n",
        "download_dir = '/tmp/'\n",
        "urllib.request.urlretrieve(data_url, data_file_name)\n",
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(download_dir)\n",
        "zip_ref.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwOEnZEPVoBK"
      },
      "source": [
        "Check that the dataset is the expected number of examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwMoZHxWOynx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f3aaa8-2af4-487d-d5fb-f0f1db6b60b3"
      },
      "source": [
        "print(\"Number of cat images:\",len(os.listdir('/tmp/PetImages/Cat/')))\n",
        "print(\"Number of dog images:\", len(os.listdir('/tmp/PetImages/Dog/')))\n",
        "\n",
        "# Expected Output:\n",
        "# Number of cat images: 12501\n",
        "# Number of dog images: 12501"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of cat images: 12501\n",
            "Number of dog images: 12501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRYQyjQhVoBO"
      },
      "source": [
        "Create some folders that will store the training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qygIo4W5O1hQ"
      },
      "source": [
        "try:\n",
        "    os.mkdir('/tmp/cats-v-dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
        "except OSError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rTFt0TDVoBR"
      },
      "source": [
        "### Split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M90EiIu0O314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2032bd-8b58-4baa-b953-6fe51ef78151"
      },
      "source": [
        "import random\n",
        "from shutil import copyfile\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    files = []\n",
        "    for filename in os.listdir(SOURCE):\n",
        "        file = SOURCE + filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    testing_length = int(len(files) - training_length)\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    testing_set = shuffled_set[:testing_length]\n",
        "\n",
        "    for filename in training_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TRAINING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "    for filename in testing_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TESTING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
        "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
        "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
        "\n",
        "split_size = .9\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "# Expected output\n",
        "# 666.jpg is zero length, so ignoring\n",
        "# 11702.jpg is zero length, so ignoring"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "666.jpg is zero length, so ignoring.\n",
            "11702.jpg is zero length, so ignoring.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N7UE18DVoBT"
      },
      "source": [
        "Check that the training and test sets are the expected lengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl8sQpM1O9xK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0167ba2-a33f-4a42-e638-42b8b48dae2a"
      },
      "source": [
        "\n",
        "print(\"Number of training cat images\", len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n",
        "print(\"Number of training dog images\", len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n",
        "print(\"Number of testing cat images\", len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n",
        "print(\"Number of testing dog images\", len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n",
        "\n",
        "# expected output\n",
        "# Number of training cat images 11250\n",
        "# Number of training dog images 11250\n",
        "# Number of testing cat images 1250\n",
        "# Number of testing dog images 1250"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training cat images 11250\n",
            "Number of training dog images 11250\n",
            "Number of testing cat images 1250\n",
            "Number of testing dog images 1250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyXvTApNVoBX"
      },
      "source": [
        "### Data augmentation (try adjusting the parameters)!\n",
        "\n",
        "Here, you'll use the `ImageDataGenerator` to perform data augmentation.  Things like rotating and flipping the existing images allows you to generate training data that is more varied, and can help the model generalize better during training.  You can also use the data generator to apply data augmentation to the validation set.\n",
        "\n",
        "Please feel free to experiment with the parameters of `ImageDataGenerator` and see how it can improve the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVO1l8vAPE14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3e1836-a8fd-494e-8cd5-cc42b6be94e3"
      },
      "source": [
        "\n",
        "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
        "# Experiment with your own parameters here to really try to drive it to 99.9% accuracy or better\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
        "# Experiment with your own parameters here to really try to drive it to 99.9% accuracy or better\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=100,\n",
        "                                                              class_mode='binary',\n",
        "                                                              target_size=(150, 150))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22498 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_oYhwvnVoBa"
      },
      "source": [
        "### Get and prepare the model\n",
        "\n",
        "You'll be using the `InceptionV3` model.  \n",
        "- Since you're making use of transfer learning, you'll load the pre-trained weights of the model.\n",
        "- You'll also freeze the existing layers so that they aren't trained on your downstream task with the cats and dogs data.\n",
        "- You'll also get a reference to the last layer, 'mixed7' because you'll add some layers after this last layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiPK1LlMOvm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c670a964-3878-44c9-ab48-66cb76057603"
      },
      "source": [
        "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "weights_file = \"inception_v3.h5\"\n",
        "urllib.request.urlretrieve(weights_url, weights_file)\n",
        "\n",
        "# Instantiate the model\n",
        "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
        "                                include_top=False,\n",
        "                                weights=None)\n",
        "\n",
        "# load pre-trained weights\n",
        "pre_trained_model.load_weights(weights_file)\n",
        "\n",
        "# freeze the layers\n",
        "for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 7, 7, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBrFMTLaVoBc"
      },
      "source": [
        "### Add layers\n",
        "Add some layers that you will train on the cats and dogs data.\n",
        "- `Flatten`: This will take the output of the `last_layer` and flatten it to a vector.\n",
        "- `Dense`: You'll add a dense layer with a relu activation.\n",
        "- 'Dense': After that, add a dense layer with a sigmoid activation.  The sigmoid will scale the output to range from 0 to 1, and allow you to interpret the output as a prediction between two categories (cats or dogs).\n",
        "\n",
        "Then create the model object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYe5dj60VoBd"
      },
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "#x = layers.Dropout(0.2)(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(pre_trained_model.input, x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiNWqyXlVoBf"
      },
      "source": [
        "### Train the model\n",
        "Compile the model, and then train it on the test data using `model.fit`\n",
        "- Feel free to adjust the number of epochs.  This project was originally designed with 20 epochs.\n",
        "- For the sake of time, you can use fewer epochs (2) to see how the code runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nxUncKWPRhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14092886-3a8f-49ff-cc70-4515c35fd143"
      },
      "source": [
        "\n",
        "# compile the model\n",
        "model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# train the model (adjust the number of epochs from 1 to improve performance)\n",
        "history = model.fit(\n",
        "            train_generator,\n",
        "            validation_data=validation_generator,\n",
        "            epochs=2,\n",
        "            verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            " 73/225 [========>.....................] - ETA: 1:50 - loss: 0.3523 - acc: 0.8788"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - 174s 773ms/step - loss: 0.2290 - acc: 0.9124 - val_loss: 0.1406 - val_acc: 0.9448\n",
            "Epoch 2/2\n",
            "225/225 [==============================] - 176s 782ms/step - loss: 0.1505 - acc: 0.9359 - val_loss: 0.0955 - val_acc: 0.9656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTITwKZ_VoBj"
      },
      "source": [
        "### Visualize the training and validation accuracy\n",
        "\n",
        "You can see how the training and validation accuracy change with each epoch on an x-y plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erDopoQ5eNL7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "e0b2886a-6cea-4cc1-9d1f-dbc6504f1b44"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkj4lcDmVoBl"
      },
      "source": [
        "### Predict on a test image\n",
        "\n",
        "You can upload any image and have the model predict whether it's a dog or a cat.\n",
        "- Find an image of a dog or cat\n",
        "- Run the following code cell.  It will ask you to upload an image.\n",
        "- The model will print \"is a dog\" or \"is a cat\" depending on the model's prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0R9fsf4w29e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "7555acbb-d844-4866-838c-daad8b05921c"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a dog\")\n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-700dcda6-f832-4a1e-9c23-7e3c1f0a1ff2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-700dcda6-f832-4a1e-9c23-7e3c1f0a1ff2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving images.jpeg to images.jpeg\n",
            "[[1.]]\n",
            "[1.]\n",
            "images.jpeg is a dog\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}